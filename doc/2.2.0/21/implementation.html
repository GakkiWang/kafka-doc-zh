<!--
 Licensed to the Apache Software Foundation (ASF) under one or more
 contributor license agreements.  See the NOTICE file distributed with
 this work for additional information regarding copyright ownership.
 The ASF licenses this file to You under the Apache License, Version 2.0
 (the "License"); you may not use this file except in compliance with
 the License.  You may obtain a copy of the License at

    http://www.apache.org/licenses/LICENSE-2.0

 Unless required by applicable law or agreed to in writing, software
 distributed under the License is distributed on an "AS IS" BASIS,
 WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 See the License for the specific language governing permissions and
 limitations under the License.
-->

<script id="implementation-template" type="text/x-handlebars-template">
    <h3><a id="networklayer" href="#networklayer">5.1 网络层</a></h3>
    <p>
        网络层相当于一个NIO服务器，这里不再详细描述。sendfile（零拷贝）实现是通过 <code>MessageSet</code> 接口提供的 <code>writeTo</code> 方法来完成的。这允许 file-backed 消息集使用更高效的 <code>transferTo</code> 实现，而不是进程内缓冲写入。线程模型（Reactor模式）是单个接受器线程和 <i>N</i> 个处理器线程，每个处理器线程处理固定数量的连接。这个设计已经过<a href="http://sna-projects.com/blog/2009/08/introducing-the-nio-socketserver-implementation">此处</a>全面测试，发现该设计能简单实施和具有快速性能。该协议非常简洁，以便将来使用其他语言实现客户端。
    </p>
    <h3><a id="messages" href="#messages">5.2 消息</a></h3>
    <p>
    消息由可变长度的header，可变长度的不透明字节数组 key 和可变长度的不透明字节数组 value 组成。header的格式将在下一节中介绍。
    让 key 和 value 不透明是正确的决定：目前在序列化库上取得了很大进展，任何特定的选择都不太适合所有使用场景。毋庸置疑，使用 Kafka 的特定应用程序可能会强制使用特定的序列化类型作为其使用的一部分。<code> RecordBatch </code>接口只是消息的迭代器，具有批量读取和写入NIO <code> Channel </code>的专用方法。
    </p>

    <h3><a id="messageformat" href="#messageformat">5.3 消息格式</a></h3>
    <p>
        消息（即 Records ）始终是批量写入。record batch 是批量消息的技术术语,它包含一条或多条 records。在不良情况下, record batch 只包含一条 record。
        Record batches 和 records 都有自己的 headers。 每种格式如下所述。</p>

    <h4><a id="recordbatch" href="#recordbatch">5.3.1 Record Batch</a></h4>
	<p> 以下是RecordBatch的磁盘格式。</p>
	<p><pre class="brush: java;">
		baseOffset: int64
		batchLength: int32
		partitionLeaderEpoch: int32
		magic: int8 (current magic value is 2)
		crc: int32
		attributes: int16
			bit 0~2:
				0: no compression
				1: gzip
				2: snappy
				3: lz4
				4: zstd
			bit 3: timestampType
			bit 4: isTransactional (0 means not transactional)
			bit 5: isControlBatch (0 means not a control batch)
			bit 6~15: unused
		lastOffsetDelta: int32
		firstTimestamp: int64
		maxTimestamp: int64
		producerId: int64
		producerEpoch: int16
		baseSequence: int32
		records: [Record]
	</pre></p>
    <p> 请注意，启用压缩后，被压缩的记录数据将在记录数量的计数后直接序列化。</p>

    <p> CRC覆盖从 attributes 到批处理结束的数据（即CRC之后的所有字节）。它位于 magic 字节之后，这意味着客户端必须在解析批处理长度和 magic 字节之间的字节之前解析 magic 字节。
        分区 leader epoch 字段不包括在 CRC 计算中，以避免当该字段分配给 broker 接收的每个批次时重新计算 CRC。CRC-32C（Castagnoli）多项式用于计算。</p>

    <p>压缩：与旧的消息格式不同，magic v2 及更高版本在清理日志时保留原始批次中的第一个和最后一个偏移/序列号。这是必需的，以便能够在重新加载日志时恢复 producer 的状态。例如，如果我们没有保留最后的序列号，那么在分区 leader 失败之后，producer 可能会看到 OutOfSequence 错误。
        必须保留基础序列号来做重复检查( broker 通过验证传入批次的第一个和最后一个序列号是否与该 producer 的最后一个序列号匹配来检查传入的重复 produce 请求)。因此,当批次中所有的记录被清理但批次数据依然保留是为了保存 producer 最后一次的序列号,日志中可能有空的数据。
        不解的是在压缩中时间戳可能不会被保留,所以如果批次中的第一条记录被压缩,时间戳也会改变。</p>

    <h5><a id="controlbatch" href="#controlbatch">5.3.1.1 控制批次</a></h5>
    <p>控制批次包含一个称为控制记录的记录。控制记录不应传递给应用程序。相反，消费者使用它们来过滤掉中止的事务性消息。</p>
    <p>  控制记录的 key 符合以下模式: </p>
    <p><pre class="brush: java">
       version: int16 (current version is 0)
       type: int16 (0 indicates an abort marker, 1 indicates a commit)
    </pre></p>
    <p>控制记录值的模式取决于类型。该值对客户端不透明。</p>

    <h4><a id="record" href="#record">5.3.2 Record</a></h4>
	<p>Record 级别的头部信息在0.11.0 版本引入。拥有 headers 的 Record 的磁盘格式如下：</p>
	<p><pre class="brush: java;">
		length: varint
		attributes: int8
			bit 0~7: unused
		timestampDelta: varint
		offsetDelta: varint
		keyLength: varint
		key: byte[]
		valueLen: varint
		value: byte[]
		Headers => [Header]
	</pre></p>
	<h5><a id="recordheader" href="#recordheader">5.3.2.1 Record Header</a></h5>
	<p><pre class="brush: java;">
		headerKeyLength: varint
		headerKey: String
		headerValueLength: varint
		Value: byte[]
	</pre></p>
    <p>我们使用与Protobuf相同的varint编码。有关后者的更多信息，请访问<a href="https://developers.google.com/protocol-buffers/docs/encoding#varints">此处</a>。
        记录中的 header 数也被编码为varint。</p>

    <h4><a id="messageset" href="#messageset">5.3.3 旧的消息格式</a></h4>
    <p>
        在Kafka 0.11之前，消息被传输并存储在<i>Message Set</i>中。在消息集中，每条消息都有自己的元数据。
        请注意，虽然消息集表示为一个数组，但它们之前没有像协议中其他数组元素那样的int32数组大小。
    </p>

    <b>Message Set:</b><br>
    <p><pre class="brush: java;">
    MessageSet (Version: 0) => [offset message_size message]
        offset => INT64
        message_size => INT32
        message => crc magic_byte attributes key value
            crc => INT32
            magic_byte => INT8
            attributes => INT8
                bit 0~2:
                    0: no compression
                    1: gzip
                    2: snappy
                bit 3~7: unused
            key => BYTES
            value => BYTES
    </pre></p>
    <p><pre class="brush: java;">
    MessageSet (Version: 1) => [offset message_size message]
        offset => INT64
        message_size => INT32
        message => crc magic_byte attributes key value
            crc => INT32
            magic_byte => INT8
            attributes => INT8
                bit 0~2:
                    0: no compression
                    1: gzip
                    2: snappy
                    3: lz4
                bit 3: timestampType
                    0: create time
                    1: log append time
                bit 4~7: unused
            timestamp =>INT64
            key => BYTES
            value => BYTES
    </pre></p>
    <p>
        在Kafka 0.10之前的版本中，唯一支持的消息格式版本（在魔术值中指示）为0。消息格式版本1在版本0.10中引入了时间戳支持。
    <ul>
        <li> 与上面的版本2类似，属性的最低位表示压缩类型。</li>
        <li>在版本1中， producer 应始终将时间戳类型位设置为0。如果 topic 配置为使用 LogAppendTime  ，
            （通过 broker  级别配置log.message.timestamp.type = LogAppendTime或 topic 级别配置 message.timestamp.type = LogAppendTime），
            broker将覆盖消息集中的时间戳类型和时间戳。</li>
        <li>必须将属性的最高位设置为0。</li>
    </ul>
    </p>
    <p>在消息格式版本0和1中，Kafka支持递归消息以启用压缩。在这种情况下，必须设置消息的属性以指明其中的一种压缩类型，并且值字段将包含使用该压缩类型的消息集。
       我们经常将嵌套消息称为“内部消息”，将包装消息称为“外部消息”。请注意，外部消息的键应为null，其偏移量将是最后一条内部消息的偏移量。
    </p>
    <p>当接收递归版本0消息时，broker 解压缩它们，并且每个内部消息被单独分配一个偏移量。
        在版本1中，为避免服务器端重新压缩，仅为包装消息分配偏移量。内部消息将具有相对偏移。
        可以使用外部消息的偏移量来计算绝对偏移量，该偏移量对应于分配给最后一个内部消息的偏移量。
    </p>

    <p>crc字段包含后续消息字节的CRC32（而不是CRC-32C）（即从 magic 字节到值）。</p>

    <h3><a id="log" href="#log">5.4 日志</a></h3>
    <p>
        具有两个分区的名为“my_topic”的 topic 的日志包含两个目录（即 <code>my_topic_0</code> 和 <code>my_topic_1</code> ），其中填充了包含该主题的消息的数据文件。志文件的格式是一系列 "log entries"（日志条目）;每个日志条目用4字节整数 <i>N</i> 来存储消息长度，后面跟着 <i>N</i>长度的消息。每个消息由64位整数 <i>offset</i> 唯一标识，该标识在该分区上发送到该主题的所有消息的流中给出该消息开始的字节位置。每个消息的磁盘格式如下所示。每个日志文件都以它包含的第一条消息的偏移量命名。所以创建的第一个文件将是00000000000.kafka，并且其它文件的整数名称大致为 <i>S</i> 来自上一个文件的字节，其中 <i>S</i> 是配置中给出的最大日志文件大小。
    </p>
    <p>
        记录(record)的确切二进制格式是版本化并作为标准接口维护，因此记录批次可以在 producer ，broker 和客户端之间传输，而无需在需要时进行重新复制或转换。上一节包含有关磁盘格式记录的详细信息。</p>
    </p>
   <p>
       使用消息偏移量作为消息ID是不常见的。我们最初的想法是使用 producer 生成的GUID，并在每个 broker 上维护从GUID到偏移的映射。但由于 consumer 必须为每个服务器维护一个ID，因此GUID的全局唯一性没有任何价值。此外，保持从随机id到偏移的映射的复杂性需要重的索引结构，其必须与磁盘同步，基本上需要完全持久的随机访问数据结构。因此，为了简化查找​​结构，我们决定使用一个简单的每分区原子计数器，它可以与分区id和节点id结合，以唯一地标识消息;这使得查找结构更简单，尽管仍然可能针对每个消费者请求进行多次搜索。然而，一旦我们确定了一个计数器，直接使用偏移量的跳转似乎很自然--毕竟对于每个分区来说它们都是一个单调递增的整数。由于消费者API隐藏了偏移量，所以这个决定最终是一个实现细节，我们采用了更高效的方法。
    </p>
    <img class="centered" src="/{{version}}/images/kafka_log.png">
    <h4><a id="impl_writes" href="#impl_writes">Writes</a></h4>
    <p>
        该日志允许串行追加到最后一个文件。当文件达到配置的大小（比如1GB）时，将转移到一个新文件。该日志有两个配置参数：<i> M </i>，它给出了在强制操作系统将文件刷新到磁盘之前要写入的消息数，以及<i> S </i>，它给出了强制刷新的时间（秒数）。这提供了在系统崩溃时最多丢失<i> M </i> 条消息或<i> S </i>秒数据的持久性保证。
    </p>
    <h4><a id="impl_reads" href="#impl_reads">Reads</a></h4>
    <p>
     通过给出消息的64位逻辑偏移和最大块的大小（<i> S </ i>字节）来完成读取。这将返回<i> S </i>  - 字节缓冲区中包含的消息的迭代器。 <i> S </i>应该比任何单个消息都大，但是如果消息异常大，则可以多次读取，每次都将缓冲区大小加倍，直到消息被成功读取。可以指定最大消息和缓冲区大小，以使服务器拒绝大于某个大小的消息，并为客户端设定读取的最大值以获取完整的消息。读缓冲区很可能以部分消息结束，这很容易通过大小分界来检测。
    </p>
    <p>
      从偏移量读取的实际过程需要首先定位存储数据的日志段文件，从全局偏移值计算文件特定的偏移量，然后从该文件偏移量中读取。搜索是通过在为每个文件维护的内存范围进行简单二分查找完成的。
    </p>
    <p>
     指定的天数内消费其数据的情况下也很有用。在这种情况下，当客户端尝试消费不存在的偏移量时，会给出OutOfRangeException，并且可以自行重置或失败。
    </p>

    <p> 以下是发送给 consumer 的结果格式。</p>

    <pre class="brush: text;">
    MessageSetSend (fetch result)

    total length     : 4 bytes
    error code       : 2 bytes
    message 1        : x bytes
    ...
    message n        : x bytes
    </pre>

    <pre class="brush: text;">
    MultiMessageSetSend (multiFetch result)

    total length       : 4 bytes
    error code         : 2 bytes
    messageSetSend 1
    ...
    messageSetSend n
    </pre>
    <h4><a id="impl_deletes" href="#impl_deletes">Deletes</a></h4>
    <p>
        数据一次删除一个日志段。日志管理器允许可插入删除策略选择哪些文件可以删除。当前策略删除修改时间超过<i> N </i>天的任何日志，尽管保留最后<i> N </i> GB的策略也可能有用。为了避免当删除操作修改段列表时锁定读取，我们使用 copy-on-write 形式的 segment 列表实现，在删除的同时它提供了一致的视图允许在多个 segment 列表视图上执行二分的搜索。
    </p>
    <h4><a id="impl_guarantees" href="#impl_guarantees">Guarantees</a></h4>
    <p>
       日志提供了配置项<i> M </i>，它控制了在强制刷盘之前的最大消息数。启动时，日志恢复线程会运行，对最新的日志段进行迭代，验证每条消息是否合法。如果消息对象的总数和偏移量小于文件的长度并且消息 payload 的 CRC32 与存储在消息中的 CRC 相匹配的话，说明这个消息对象是合法的。如果检测到损坏，日志会在最后一个合法 offset 处截断。
    </p>
    <p>
        请注意，有两种损坏必须处理：由于崩溃导致的未写入的数据块的丢失和将无意义的数据块添加到文件。原因是：通常操作系统不能保证文件 inode 和实际数据块之间的写入顺序，除此之外，如果在块数据被写入之前，文件 inode 已更新为新的大小，若此时系统崩溃，文件不会得到有意义的数据，则会导致数据丢失。
    </p>

    <h3><a id="distributionimpl" href="#distributionimpl">5.5 分布式</a></h3>
    <h4><a id="impl_offsettracking" href="#impl_offsettracking">Consumer Offset Tracking(消费者offset跟踪)</a></h4>
    <p>
        Kafka consumer 跟踪它在每个分区中消耗的最大偏移量，并且能够提交偏移量，以便在重新启动时可以从这些偏移量中恢复。
        Kafka提供了为给定 consumer 组的所有偏移存储在一个叫组协调器的 broker（针对该组） 的选项。 即，该 consumer 组中的任何 consumer 实例应将其偏移提交和提取发送给该组协调器（broker）。
        consumer组根据其组名分配给协调器。consumer 可以通过向任何Kafka broker 发出FindCoordinatorRequest并读取包含协调器详细信息的 FindCoordinatorResponse 来查找其协调器。然后，consumer 可以继续从协调 broker 上提交或获取偏移量。
        在协调器移动的情况下，consumer 将需要重新发现协调器。 偏移提交可以由 consumer 实例自动或手动完成。
    </p>

    <p>
        当组协调器收到 OffsetCommitRequest 时，它会将请求附加到名为<i> __ consumer_offsets </i>的特殊<a href="#compaction">压缩</a> Kafka topic中。
        仅在偏移主题的所有副本都接收到偏移量后，broker 才会向 consumer 发送成功的偏移提交响应。
        如果偏移量在超时（可配置）范围内无法复制，则偏移提交将失败，并且 consumer 可以在失败后重试提交。
        代理定期压缩偏移 topic，因为它只需要维护每个分区的最新偏移提交。
        协调器还将偏移缓存在内存表中，以便提供快速地偏移提取。
    </p>

    <p>
        当协调器接收到偏移量获取请求时，它只返回来自偏移量缓存的最后提交的偏移量向量。
        如果协调器刚刚启动或者它刚刚成为新的一组 consumer 组的协调者（通过成为偏移 topic 分区的领导者），它可能需要将偏移 topic 分区加载到缓存中。
        在这种情况下，偏移量提取将失败并出现CoordinatorLoadInProgressException，并且消费者可能会在失败后重试OffsetFetchRequest。
    </p>

    <h4><a id="impl_zookeeper" href="#impl_zookeeper">ZooKeeper 目录</a></h4>
    <p>
     以下给出了用于 consumer 和 broker 之间协调的 ZooKeeper 结构和算法。
    </p>

    <h4><a id="impl_zknotation" href="#impl_zknotation">注释</a></h4>
    <p>
        当一个path中的元素表示为[XYZ]，这意味着xyz的值不是固定的，实际上每个xyz的值可能是Zookeeper的znode，例如`/topic/[topic]`是一个目录，/topic包含一个子目录(每个topic名称)。数字的范围如[0...5]来表示子目录0，1，2，3，4。箭头`->`用于表示znode的内容，例如:/hello->world表示znode /hello包含值”world”。
    </p>

    <h4><a id="impl_zkbroker" href="#impl_zkbroker">Broker 节点注册表</a></h4>
    <pre class="brush: json;">
    /brokers/ids/[0...N] --> {"jmx_port":...,"timestamp":...,"endpoints":[...],"host":...,"version":...,"port":...} (ephemeral node)
    </pre>
    <p>
        这是当前所有broker的节点列表，其中每个提供了一个唯一的逻辑 broker 的id 用于向它的 consumer 标识（必须作为配置的一部分）。在启动时，broker 节点通过在/brokers/ids/下用逻辑broker id创建一个znode来注册它自己。逻辑 broker id的目的是当broker移动到不同的物理机器时，而不会影响 consumer 。尝试注册一个已存在的broker id时将返回错误（列如，因为2个server配置了相同的broker id）。
    </p>
    <p>
        由于 broker 使用 ephemeral znode在ZooKeeper中注册自己，因此该注册是动态的，并且如果 broker 关闭或死亡（因此通知 consumer不再可用），该注册将消失。
    </p>
    <h4><a id="impl_zktopic" href="#impl_zktopic">Broker Topic 注册表</a></h4>
    <pre class="brush: json;">
    /brokers/topics/[topic]/partitions/[0...N]/state --> {"controller_epoch":...,"leader":...,"version":...,"leader_epoch":...,"isr":[...]} (ephemeral node)
    </pre>

    <p>
        每个 broker 在其维护的主题下注册自己，并存储该 topic 的分区数。
    </p>

    <h4><a id="impl_clusterid" href="#impl_clusterid">Cluster Id</a></h4>

    <p>
        cluster id 是分配给Kafka集群的唯一且不可变的标识符。cluster id最多可包含22个字符，允许的字符由正则表达式  [a-zA-Z0-9_\-]+ [a-zA-Z0-9_\-]+ 定义，它对应于URL安全的Base64变体使用的字符，没有填充。 从概念上讲，它是第一次启动集群时自动生成的。
    </p>
    <p>
        在实现方面，它是在第一次成功启动版本为0.10.1或更高版本的 broker 时生成的。broker 在启动期间尝试从<code> /cluster/id </code> znode获取 cluster ID。 如果znode不存在，broker 将生成新的cluster id ，并使用此id创建znode。
    </p>

    <h4><a id="impl_brokerregistration" href="#impl_brokerregistration">Broker 节点注册</a></h4>

    <p>
        broker 节点基本上是独立的，因此它们只发布有关它们的内容的信息。当 broker 加入时，它会在 broker 节点注册表目录下注册自己，并写入有关其主机名和端口的信息。broker 还在 broker 主题注册表中注册现有主题及其逻辑分区的列表。 在broker上创建新主题时，会动态注册这些主题。
    </p>
</script>

<div class="p-implementation"></div>
